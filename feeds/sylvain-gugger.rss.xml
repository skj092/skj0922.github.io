<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Another data science student's blog - Sylvain Gugger</title><link>/</link><description></description><lastBuildDate>Thu, 03 May 2018 16:22:00 -0400</lastBuildDate><item><title>Deep Painterly Harmonization</title><link>/deep-painterly-harmonization.html</link><description>&lt;p class="first last"&gt;In this article we'll decode the research article with the same name and get some cool results integrating random objects in paintings while preserving their style.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Thu, 03 May 2018 16:22:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-05-03:/deep-painterly-harmonization.html</guid><category>Deep Learning</category><category>Style Transfer</category></item><item><title>Pointer cache for Language Model</title><link>/pointer-cache-for-language-model.html</link><description>&lt;p class="first last"&gt;You can easily boost the performance of a language model based on RNNs by adding a pointer cache on top of it. The idea was introduce by Grave et al. and their results showed how this simple technique can make your perplexity decrease by 10 points without additional training. This sounds exciting, so let's see what this is all about and implement that in pytorch with the fastai library.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Thu, 26 Apr 2018 17:43:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-04-26:/pointer-cache-for-language-model.html</guid><category>Deep Learning</category><category>NLP</category></item><item><title>Recurrent Neural Network</title><link>/recurrent-neural-network.html</link><description>&lt;p class="first last"&gt;In Natural Language Processing, traditional neural networks struggle to properly execute the task we give them. To predict the next work in a sentence for instance, or grasp its meaning to somehow classify it, you need to have a structure that can keeps some memory of the words it saw before. That's why Recurrent Neural Network have been designed to do, and we'll look into them in this article.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Sat, 14 Apr 2018 16:31:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-04-14:/recurrent-neural-network.html</guid><category>Deep Learning</category><category>NLP</category></item><item><title>The 1cycle policy</title><link>/the-1cycle-policy.html</link><description>&lt;p class="first last"&gt;Properly setting the hyper-parameters of a neural network can be challenging, fortunately, there are some recipe that can help.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Sat, 07 Apr 2018 15:23:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-04-07:/the-1cycle-policy.html</guid><category>Deep Learning</category><category>SGD</category><category>Learning Rate</category></item><item><title>Convolution in depth</title><link>/convolution-in-depth.html</link><description>&lt;p class="first last"&gt;CNNs (Convolutional Neural Network) are the most powerful networks used in computer vision. Let's see what a convolutional layer is all about, from the definition to the implementation in numpy, even with the back propagation.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Thu, 05 Apr 2018 11:03:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-04-05:/convolution-in-depth.html</guid><category>Deep Learning</category><category>Convolution</category></item><item><title>SGD Variants</title><link>/sgd-variants.html</link><description>&lt;p class="first last"&gt;Let's get a rapid overview and implementations of the common variants of SGD.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Thu, 29 Mar 2018 16:35:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-03-29:/sgd-variants.html</guid><category>SGD</category><category>Deep Learning</category></item><item><title>A simple neural net in numpy</title><link>/a-simple-neural-net-in-numpy.html</link><description>&lt;p class="first last"&gt;Now that we have seen how to build a neural net in pytorch, let's try to take it a step further and try to do the same thing in numpy.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Tue, 20 Mar 2018 16:15:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-03-20:/a-simple-neural-net-in-numpy.html</guid><category>Neural Net</category><category>Back Propagation</category></item><item><title>How Do You Find A Good Learning Rate</title><link>/how-do-you-find-a-good-learning-rate.html</link><description>&lt;p class="first last"&gt;This is the main hyper-parameter to set when we train a neural net, but how do you determine the best value? Here's a technique to quickly decide on one.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Tue, 20 Mar 2018 16:15:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-03-20:/how-do-you-find-a-good-learning-rate.html</guid><category>SGD</category><category>Learning Rate</category></item><item><title>A Neural Net In Pytorch</title><link>/a-neural-net-in-pytorch.html</link><description>&lt;p class="first last"&gt;The theory is all really nice, but let's actually build a neural net and train it! We'll see how a simple neural net with one hidden layer can learn to recognize digits very efficiently.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Fri, 16 Mar 2018 10:13:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-03-16:/a-neural-net-in-pytorch.html</guid><category>Neural net</category><category>Pytorch</category><category>Deep learning</category></item><item><title>What Is Deep Learning?</title><link>/what-is-deep-learning.html</link><description>&lt;p class="first last"&gt;What is deep learning? It's a class of algorithms where you train something called a neural net to complete a specific task. Let's begin with a general overview and we will dig into the details in subsequent articles.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Tue, 13 Mar 2018 17:20:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-03-13:/what-is-deep-learning.html</guid><category>Neural Net</category><category>SGD</category><category>Deep Learning</category></item><item><title>Why Write A Blog?</title><link>/why-write-a-blog.html</link><description>&lt;p&gt;I've just been accepted to follow the 2018 version of Deep learning - part 2 on &lt;a class="reference external" href="http://fast.ai/"&gt;fast.ai&lt;/a&gt;, and I'm pretty excited about it. As I'm reaching the stage at which this is becoming more
than a hobby and the plan is to switch careers to Data Science, I've taken the â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Mon, 12 Mar 2018 16:57:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-03-12:/why-write-a-blog.html</guid></item></channel></rss>